Lecture_12_Spark_Dataframes/
├── Introduction_to_Spark_and_Hadoop
│   └── Explanation:
│       Apache Hadoop and Apache Spark are two core frameworks for big data processing.
│       - **Hadoop** focuses on distributed storage (HDFS) and batch processing using MapReduce.
│       - **Spark** is a fast, in-memory data processing engine that can run on top of Hadoop, offering more flexibility and speed.
│
│       Hadoop processes data by dividing it into blocks, distributing tasks across machines using MapReduce.
│       Spark, in contrast, uses Resilient Distributed Datasets (RDDs) and DataFrames for faster, iterative processing.
│
│       **Key Differences:**
│       - Hadoop stores intermediate results on disk; Spark keeps data in memory.
│       - Spark supports SQL, streaming, machine learning, and graph processing natively.
│
│       Visualization:
│       Hadoop Workflow:
│           Input Data -> HDFS -> MapReduce -> Output
│
│       Spark Workflow:
│           Input Data -> RDD/DataFrame -> In-Memory Processing -> Output
│
│       Related_Concepts/
│       ├── HDFS
│       │   └── Explanation:
│       │       Hadoop Distributed File System splits large files into blocks (default 128MB) and distributes them across nodes.
│       │       It ensures fault tolerance via data replication.
│       │
│       └── MapReduce_vs_Spark
│           └── Explanation:
│               MapReduce is disk-based and suited for batch processing.
│               Spark enhances performance by reducing disk I/O through in-memory computation.
│
│               [External Standard Explanation]
│               Spark can outperform Hadoop by 10-100x for certain workloads due to this design.
├── Spark_Architecture_and_Components
│   └── Explanation:
│       Apache Spark's architecture is based on a **master-slave** model:
│       - **Driver Program**: The central coordinator. It defines transformations/actions and manages the SparkContext.
│       - **Cluster Manager**: Allocates resources (e.g., YARN, Mesos, or standalone).
│       - **Workers/Executors**: Perform tasks assigned by the driver. Executors run computations and store data.
│
│       The core abstraction in Spark is the **RDD (Resilient Distributed Dataset)**, later enhanced by **DataFrames** and **Datasets**.
│
│       **Execution Flow:**
│       1. Driver sends tasks to executors.
│       2. Tasks operate on partitions of data.
│       3. Results are sent back to the driver.
│
│       Visualization:
│       [Driver Program]
│             │
│             ▼
│       [Cluster Manager]
│             │
│       ┌─────┴─────┐
│       │           │
│   [Executor 1] [Executor 2]
│       │           │
│   [Task 1]     [Task 2]
│
│       Related_Concepts/
│       ├── SparkContext
│       │   └── Explanation:
│       │       Entry point for Spark functionality. It connects the driver to the cluster.
│       │
│       ├── Executors
│       │   └── Explanation:
│       │       JVM processes responsible for running tasks and storing data for applications.
│       │
│       └── Cluster_Manager_Types
│           └── Explanation:
│               Spark supports:
│               - **Standalone**: Built-in cluster manager.
│               - **YARN**: Hadoop’s resource manager.
│               - **Mesos**: General cluster manager.
├── RDDs_and_DataFrames
│   └── Explanation:
│       **RDD (Resilient Distributed Dataset)**:
│       - Fundamental data structure in Spark.
│       - Immutable, distributed collections of objects.
│       - Supports two operations:
│         1. **Transformations** (lazy, e.g., map, filter)
│         2. **Actions** (trigger execution, e.g., collect, count)
│
│       **DataFrames**:
│       - Higher-level abstraction built on top of RDDs.
│       - Similar to a table in a relational database or a Pandas DataFrame.
│       - Optimized using Spark's Catalyst optimizer.
│
│       **Key Differences:**
│       - RDD: Low-level, type-safe, no schema.
│       - DataFrame: High-level, schema-based, optimized execution.
│
│       Visualization:
│       Data Abstraction Levels:
│           RDD  -->  DataFrame  -->  Dataset (typed API, mainly in Scala/Java)
│
│       Example_Code/
│       ├── Create_RDD_and_DataFrame.py
│       │   ```python
│       │   # Create RDD
│       │   rdd = spark.sparkContext.parallelize([("Alice", 29), ("Bob", 31)])
│       │
│       │   # Convert to DataFrame
│       │   df = rdd.toDF(["Name", "Age"])
│       │
│       │   df.show()
│       │   ```
│       │   Dry Run:
│       │   Input: [("Alice", 29), ("Bob", 31)]
│       │   Output:
│       │   +-----+---+
│       │   | Name|Age|
│       │   +-----+---+
│       │   |Alice| 29|
│       │   |  Bob| 31|
│       │   +-----+---+
│
│       Related_Concepts/
│       ├── Lazy_Evaluation
│       │   └── Explanation:
│       │       Spark delays execution until an action is called, optimizing the computation DAG.
│       │
│       └── Catalyst_Optimizer
│           └── Explanation:
│               Spark's query optimizer that transforms logical plans into efficient physical execution plans.
├── Spark_SQL_and_Querying
│   └── Explanation:
│       **Spark SQL** is a module in Apache Spark for structured data processing.
│       - Allows querying DataFrames using SQL syntax.
│       - Integrates seamlessly with existing Spark APIs (Python, Java, Scala).
│       - Leverages the Catalyst Optimizer for efficient execution.
│
│       You can register DataFrames as temporary views and run SQL queries directly.
│
│       Example_Code/
│       ├── SQL_Query_on_DataFrame.py
│       │   ```python
│       │   data = [("Alice", 29), ("Bob", 31), ("Alice", 35)]
│       │   df = spark.createDataFrame(data, ["Name", "Age"])
│       │
│       │   # Register as SQL temporary view
│       │   df.createOrReplaceTempView("people")
│       │
│       │   # Run SQL Query
│       │   result = spark.sql("SELECT Name, AVG(Age) as Avg_Age FROM people GROUP BY Name")
│       │   result.show()
│       │   ```
│       │   Dry Run:
│       │   Input Data:
│       │   +-----+---+
│       │   | Name|Age|
│       │   +-----+---+
│       │   |Alice| 29|
│       │   |  Bob| 31|
│       │   |Alice| 35|
│       │   +-----+---+
│       │
│       │   Output:
│       │   +-----+-------+
│       │   | Name|Avg_Age|
│       │   +-----+-------+
│       │   |Alice|   32.0|
│       │   |  Bob|   31.0|
│       │   +-----+-------+
│
│       Visualization:
│       DataFrame  →  Temporary View  →  SQL Query Execution  →  Optimized Result
│
│       Related_Concepts/
│       ├── Temporary_Views
│       │   └── Explanation:
│       │       Logical views created from DataFrames for running SQL queries. They exist for the session duration.
│       │
│       └── Catalyst_Optimizer
│           └── Explanation:
│               Automatically optimizes SQL queries for performance, reordering filters, projections, etc.
├── DataFrame_Operations
│   └── Explanation:
│       **DataFrames** in Spark support a rich set of operations similar to SQL and Pandas.
│       These operations are categorized into:
│
│       1. **Transformations** (Lazy Operations):
│          - Examples: `select()`, `filter()`, `groupBy()`, `withColumn()`
│          - These define a computation but don’t execute immediately.
│
│       2. **Actions** (Trigger Execution):
│          - Examples: `show()`, `collect()`, `count()`, `write()`
│          - These trigger the actual computation and return results.
│
│       Example_Code/
│       ├── Transformations_and_Actions.py
│       │   ```python
│       │   df = spark.read.csv("cars.csv", header=True, inferSchema=True)
│       │
│       │   # Transformation: Filter cars with price > 20000
│       │   filtered_df = df.filter(df.Price > 20000)
│       │
│       │   # Transformation: Select specific columns
│       │   selected_df = filtered_df.select("Make", "Model", "Price")
│       │
│       │   # Action: Show top 5 results
│       │   selected_df.show(5)
│       │   ```
│       │   Dry Run:
│       │   Input: cars.csv (Sample Rows)
│       │   +-------+--------+-------+
│       │   |  Make | Model  | Price |
│       │   +-------+--------+-------+
│       │   | Toyota| Camry  | 24000 |
│       │   | Honda | Accord | 22000 |
│       │   | Ford  | Focus  | 18000 |
│       │   +-------+--------+-------+
│       │
│       │   After `filter`: Removes Ford Focus (Price ≤ 20000)
│       │   After `select`: Keeps only Make, Model, Price columns
│       │
│       │   Output:
│       │   +-------+--------+-------+
│       │   |  Make | Model  | Price |
│       │   +-------+--------+-------+
│       │   | Toyota| Camry  | 24000 |
│       │   | Honda | Accord | 22000 |
│       │   +-------+--------+-------+
│
│       Related_Concepts/
│       ├── Lazy_Evaluation
│       │   └── Explanation:
│       │       Transformations are recorded as a logical plan and only executed when an action is called.
│       │
│       └── DAG_Scheduler
│           └── Explanation:
│               Spark builds a Directed Acyclic Graph (DAG) of stages and tasks before execution.
│
│       Visualization:
│       Operations Flow:
│           Read CSV
│               │
│           ──► Filter (Price > 20000)
│               │
│           ──► Select (Make, Model, Price)
│               │
│           ──► Action: show()
│               │
│           ─► Trigger DAG Execution → Result
├── Data_Sources_and_Interoperability
│   └── Explanation:
│       Spark DataFrames can seamlessly read from and write to various data sources:
│
│       **Common Data Sources:**
│       - **CSV**: Comma-Separated Values.
│       - **JSON**: JavaScript Object Notation.
│       - **Parquet**: Columnar storage format, optimized for performance.
│       - **ORC**, **Avro**, Databases via JDBC.
│
│       **Reading Data Example:**
│       ```python
│       # Reading CSV
│       df_csv = spark.read.option("header", True).csv("cars.csv")
│
│       # Reading JSON
│       df_json = spark.read.json("data.json")
│
│       # Reading Parquet
│       df_parquet = spark.read.parquet("data.parquet")
│       ```
│       Dry Run:
│       Files:
│       - cars.csv → Loaded as DataFrame with inferred schema.
│       - data.json → Loaded as structured DataFrame.
│       - data.parquet → Efficiently loaded with columnar optimization.
│
│       **Writing Data Example:**
│       ```python
│       df_csv.write.json("output_folder/json_output")
│       df_csv.write.parquet("output_folder/parquet_output")
│       ```
│       Dry Run:
│       DataFrame → Saved as JSON and Parquet formats in respective folders.
│
│       **Interoperability:**
│       Spark allows conversion between RDDs, DataFrames, and Datasets (Scala/Java).
│
│       Example_Code/
│       ├── DataFrame_to_RDD.py
│       │   ```python
│       │   rdd = df_csv.rdd
│       │   ```
│       │   Dry Run:
│       │   DataFrame rows converted into RDD of Row objects.
│
│       ├── RDD_to_DataFrame.py
│       │   ```python
│       │   rdd = spark.sparkContext.parallelize([("Alice", 29)])
│       │   df = rdd.toDF(["Name", "Age"])
│       │   ```
│       │   Dry Run:
│       │   RDD converted to structured DataFrame with schema ["Name", "Age"].
│
│       Related_Concepts/
│       ├── Schema_Inference
│       │   └── Explanation:
│       │       Spark can automatically detect data types when reading files like CSV and JSON.
│       │
│       └── Parquet_Format
│           └── Explanation:
│               A columnar storage format providing efficient data compression and encoding schemes.
│               [External Standard Explanation]
├── Performance_Optimization_in_Spark
│   └── Explanation:
│       Spark provides several mechanisms to optimize performance, ensuring efficient execution of big data workloads.
│
│       **Key Optimization Techniques:**
│
│       1. **Catalyst Optimizer**:
│          - Optimizes logical and physical query plans.
│          - Reorders filters, projections, and simplifies expressions.
│
│       2. **Tungsten Execution Engine**:
│          - Low-level memory management.
│          - CPU and cache-aware computation.
│          - Reduces JVM overhead by using binary processing.
│
│       3. **Persisting and Caching**:
│          - Use `cache()` or `persist()` to store intermediate DataFrames/RDDs in memory.
│          - Prevents recomputation in iterative operations.
│
│       4. **Partitioning**:
│          - Proper partitioning ensures parallelism.
│          - Use `repartition()` or `coalesce()` to manage data distribution.
│
│       5. **Broadcast Variables**:
│          - Efficiently share small lookup tables across executors to avoid data shuffling.
│
│       Example_Code/
│       ├── Caching_and_Persistence.py
│       │   ```python
│       │   df = spark.read.parquet("bigdata.parquet")
│       │
│       │   # Cache the DataFrame in memory
│       │   df.cache()
│       │
│       │   # Perform multiple actions without recomputation
│       │   df.count()
│       │   df.select("columnA").show()
│       │   ```
│       │   Dry Run:
│       │   - DataFrame loaded and cached.
│       │   - First action triggers computation & caching.
│       │   - Subsequent actions reuse cached data.
│
│       ├── Broadcast_Variable.py
│       │   ```python
│       │   small_df = spark.read.csv("lookup.csv")
│       │   broadcast_var = spark.sparkContext.broadcast(small_df.collectAsMap())
│       │   ```
│       │   Dry Run:
│       │   - Small lookup table broadcasted to all executors.
│       │   - Reduces need for expensive joins.
│
│       Visualization:
│       Optimization Layers:
│           User Code
│               │
│           ──► Catalyst Optimizer
│               │
│           ──► Tungsten Engine (Memory + CPU Optimization)
│               │
│           ──► Execution
│
│       Related_Concepts/
│       ├── Shuffle
│       │   └── Explanation:
│       │       Data movement across partitions/nodes. Minimizing shuffles is key to performance.
│       │
│       ├── Coalesce_vs_Repartition
│       │   └── Explanation:
│       │       - `repartition(n)`: Increases or evenly redistributes partitions.
│       │       - `coalesce(n)`: Reduces partitions with minimal data movement.
│       │
│       └── Tungsten_Project
│           └── Explanation:
│               [External Standard Explanation]
│               A Spark initiative for optimizing memory and CPU efficiency through binary processing and off-heap memory management.
├── Use_Cases_Best_Practices_and_Resources
│   └── Explanation:
│       Apache Spark is widely adopted due to its versatility, speed, and ease of integration with big data ecosystems.
│
│       **Common Use Cases:**
│       - **ETL (Extract, Transform, Load)**: Process large-scale data pipelines efficiently.
│       - **Data Warehousing**: Using Spark SQL for querying structured data.
│       - **Machine Learning**: Leveraging MLlib for scalable machine learning tasks.
│       - **Real-Time Stream Processing**: Using Spark Streaming for handling live data feeds.
│       - **Graph Processing**: Utilizing GraphX for graph-parallel computations.
│
│       **Best Practices:**
│       1. **Minimize Shuffles**:
│          - Design queries and transformations to reduce data movement.
│
│       2. **Leverage Caching Wisely**:
│          - Cache only when reused multiple times; avoid unnecessary memory usage.
│
│       3. **Use DataFrames over RDDs**:
│          - Benefit from Catalyst optimization and better performance.
│
│       4. **Optimize Partitioning**:
│          - Ensure balanced partition sizes for parallelism.
│
│       5. **Broadcast Small Datasets**:
│          - Avoid large joins by broadcasting lookup tables.
│
│       Example_Scenario/
│       ├── ETL_Pipeline.py
│       │   ```python
│       │   raw_df = spark.read.json("logs.json")
│       │   cleaned_df = raw_df.filter(raw_df.status == "SUCCESS").cache()
│       │   cleaned_df.write.parquet("clean_logs.parquet")
│       │   ```
│       │   Dry Run:
│       │   - Reads JSON logs.
│       │   - Filters successful entries.
│       │   - Caches filtered data and writes to Parquet for optimized storage.
│
│       Related_Concepts/
│       ├── Spark_Streaming
│       │   └── Explanation:
│       │       Enables processing of real-time data streams using micro-batch architecture.
│       │
│       ├── MLlib
│       │   └── Explanation:
│       │       Spark’s machine learning library supporting classification, regression, clustering, etc.
│       │
│       └── GraphX
│           └── Explanation:
│               Spark’s API for graph processing and graph-parallel computations.
│
│       Resources/
│       ├── Official_Documentation
│       │   └── https://spark.apache.org/docs/latest/
│       │
│       ├── Apache_Spark_GitHub
│       │   └── https://github.com/apache/spark
│       │
│       └── Books_and_Tutorials
│           └── "Learning Spark" by Holden Karau et al.
│               [External Standard Explanation]
│               A comprehensive guide to mastering Spark’s core concepts and advanced features.
